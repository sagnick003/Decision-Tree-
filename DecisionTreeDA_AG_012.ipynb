{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-012\n",
        "\n",
        "Decision Tree | Assignment\n",
        "\n",
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans - A Decision Tree (DT) is a non-parametric supervised learning algorithm used for both classification and regression tasks.In the context of classification (often called a Classification Tree), its goal is to create a model that predicts the class or category of a target variable by learning simple decision rules inferred from the data features.The structure of a decision tree is hierarchical and resembles an upside-down tree or a flowchart, making it highly interpretable and easy to visualize.\n",
        "\n",
        "Key Components of a Decision Tree -\n",
        "\n",
        "Root Node: Represents the entire dataset, which is then split into two or more homogeneous sets.\n",
        "\n",
        "Internal Nodes (Decision Nodes): Represent a test on a specific feature (attribute).\n",
        "\n",
        "Branches: Represent the outcome of the test or decision at the node.\n",
        "\n",
        "Leaf Nodes (Terminal Nodes): Represent the final class label or outcome.No further splitting occurs here.\n",
        "\n",
        "How a Decision Tree Works for Classification -\n",
        "\n",
        "The construction of a Classification Tree employs a top-down, greedy approach known as Recursive Partitioning.The process involves repeatedly splitting the data into purer subsets based on the features, until a stopping criterion is met.Starting at the Root NodeThe process begins with the entire dataset at the root node.The algorithm must decide which feature to use for the first split and what the split point should be.\n",
        "\n",
        "Feature Selection and Splitting -\n",
        "\n",
        "At every node, the algorithm evaluates all available features and potential split points to find the \"best\" split.The goal of the split is to separate the data into subsets that are as homogeneous (or \"pure\") as possible with respect to the target variable's class labels.This \"best\" split is determined by an Attribute Selection Measure (ASM), which quantifies the impurity or randomness of the node's class distribution.Common ASMs used for classification include:Gini Impurity: Measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the distribution of classes in the node.The goal is to minimize Gini impurity.$$\\text{Gini}(t) = 1 - \\sum_{i=1}^{c} [P(i|t)]^2$$where $P(i|t)$ is the probability of class $i$ at node $t$.Information Gain: Measures the reduction in Entropy (a measure of disorder/uncertainty) achieved by a split.The goal is to maximize the Information Gain.$$\\text{Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)$$where $S$ is the set of data, $A$ is the attribute being split on, and $S_v$ is the subset for value $v$.The feature that results in the highest Information Gain or lowest Gini Impurity is chosen as the split for the current node.\n",
        "\n",
        "3. Recursive Process and Stopping Criteria -\n",
        "\n",
        "The process of selecting the best split and partitioning the data is applied recursively to each newly created child node.This \"divide and conquer\" approach continues until a stopping criterion is met, which could be:All data points in a node belong to the same class (perfect purity).A pre-defined maximum depth of the tree is reached.The number of data points in a node falls below a minimum threshold.No split is found that significantly reduces impurity.\n",
        "\n",
        "4. Prediction -\n",
        "\n",
        "Once the tree is built (trained), a new, unclassified data point is classified by starting at the root node and following the branches based on the feature values of the data point, until it reaches a leaf node.The class label assigned to that leaf node (usually the majority class of the training samples that ended up there) is the model's prediction for the new data point.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans - Decision Tree algorithms use Impurity Measures to quantify the \"mixed-up\" nature of class labels within a node. The primary objective when building a tree is to find splits that minimize impurity in the child nodes, thereby creating subsets that are as pure (homogeneous) as possible.\n",
        "\n",
        "Gini ImpurityGini Impurity is a measure of the likelihood of an incorrect classification of a new instance if that instance were randomly classified according to the distribution of class labels in the node.3Concept: It measures the probability of misclassifying a randomly chosen element in the dataset.4Range: It ranges from 0 to 0.5 (for a two-class problem).50 (Pure): All elements belong to the same class (perfectly pure node).60.5 (Maximum Impurity): Elements are equally distributed among all classes (maximum disorder).Formula: For a node 7$t$ with 8$c$ classes, where 9$P(i|t)$ is the probability of class 10$i$ at node 11$t$:12$$\\text{Gini}(t) = 1 - \\sum_{i=1}^{c} [P(i|t)]^2$\n",
        "\n",
        "Entropy -\n",
        "\n",
        "Entropy is a concept derived from information theory that measures the disorder or uncertainty within a set of data.\n",
        "\n",
        "Concept: It quantifies the amount of \"surprise\" or randomness in the class distribution of a node.Higher entropy means more uncertainty.\n",
        "\n",
        "Range: It ranges from 0 to 1 (for a two-class problem, but can be greater than 1 for more classes, e.g., $\\log_2(c)$).0 (Pure): All elements belong to the same class (no uncertainty).(Maximum Impurity): Elements are equally distributed among all classes (maximum uncertainty).\n",
        "\n",
        "Formula: For a node $t$ with $c$ classes, where $P(i|t)$ is the probability of class $i$ at node t:-\n",
        "$$\\text{Entropy}(t) = - \\sum_{i=1}^{c} P(i|t) \\log_2 P(i|t)$$\n",
        "\n",
        "\n",
        "Impact on Decision Tree Splits\n",
        "\n",
        "The way these measures are used to choose the best split is by calculating the reduction in impurity after a potential split, for every feature.Gini Impurity Criterion (CART Algorithm)The decision tree algorithm (like CART) aims to choose the split that results in the minimum weighted Gini Impurity of the resulting child nodes.$$\\text{Gini}_{\\text{split}} = \\frac{n_{\\text{left}}}{n} \\text{Gini}_{\\text{left}} + \\frac{n_{\\text{right}}}{n} \\text{Gini}_{\\text{right}}$$The split that gives the lowest 19$\\text{Gini}_{\\text{split}}$ value is chosen.Entropy Criterion (ID3/C4.5 Algorithms)The entropy criterion uses Information Gain (IG), which is the difference between the parent node's entropy and the weighted average entropy of its child nodes.$$\\text{Information Gain} = \\text{Entropy}_{\\text{parent}} - \\sum_{j} \\frac{n_j}{n} \\text{Entropy}_j$$where $j$ iterates over the child nodes, and $n_j$ is the number of samples in child node $j$.The split that yields the maximum Information Gain is chosen.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans - Pruning is a technique essential for simplifying Decision Trees and preventing overfitting, which occurs when a model learns the training data (including noise) too well, leading to poor generalization on new, unseen data. The two main approaches are distinguished by when the simplification occurs.\n",
        "\n",
        "Pre-Pruning (Early Stopping)-\n",
        "\n",
        "Pre-Pruning, or Early Stopping, involves stopping the growth of the decision tree prematurely, during the training phase itself.\n",
        "\n",
        "Feature\t                                     Description\n",
        "\n",
        "When it Occurs\t                      During the tree construction.\n",
        "Mechanism\t                      The algorithm uses stopping criteria (hyperparameters) to decide not to split a node further.\n",
        "\n",
        "Common Criteria\n",
        "\n",
        "Setting a maximum tree depth (max_depth), requiring a minimum number of samples for a split (min_samples_split), or stopping if the impurity reduction is below a certain threshold.\n",
        "\n",
        "Practical Advantage of Pre-Pruning -\n",
        "\n",
        "The main advantage is computational efficiency and speed. Since the tree is prevented from growing to its full, potentially very large size, the training time is significantly reduced, making it ideal for large datasets or real-time systems where rapid training is crucial.\n",
        "\n",
        "Post-Pruning (Backward Pruning) -\n",
        "\n",
        "Post-Pruning involves first allowing the decision tree to fully grow (often resulting in an overfitted tree) and then systematically trimming back unnecessary branches or subtrees.\n",
        "\n",
        "Feature\t                   Description\n",
        "\n",
        "When it Occurs\t        After the tree is fully constructed.\n",
        "\n",
        "Mechanism\t          Branches are removed (replaced by a leaf node) if the resulting simpler tree's accuracy on a separate validation set is not significantly reduced.\n",
        "\n",
        "Common Techniques\t  Reduced Error Pruning and Cost-Complexity Pruning (using a parameter like ccp_alpha in scikit-learn).\n",
        "\n",
        "Practical Advantage of Post-Pruning -\n",
        "\n",
        "The main advantage is greater accuracy and better generalization. Since the algorithm allows the tree to explore all possible splits first, it avoids the \"Horizon Effect\" of pre-pruning, where an early, seemingly unpromising split is discarded but might have led to a highly accurate subtree later on. Post-pruning ensures the model finds the most optimal balance between complexity and predictive power.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Ans - Information Gain (IG) is a metric used in Decision Tree algorithms (like ID3 and C4.5) to quantify the effectiveness of an attribute in classifying the data.It measures the reduction in uncertainty (or impurity) after a dataset is split based on that attribute.In essence, IG tells you how much \"information\" a feature provides about the class label.The FormulaInformation Gain is calculated as the difference between the Entropy of the parent node and the weighted average Entropy of the child nodes created by the split:$$\\text{IG}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)$$Where:$\\text{IG}(S, A)$: The Information Gain for splitting dataset $S$ using attribute $A$.$\\text{Entropy}(S)$: The impurity of the parent set 8$S$ before the split.$\\text{Values}(A)$: The set of all possible values for attribute $A$.$|S_v|/|S|$: The weight of each child node, which is the proportion of data points that take on value $v$ for attribute $A$.$\\text{Entropy}(S_v)$: The impurity of the subset $S_v$ (the child node).\n",
        "\n",
        "Importance for Choosing the Best Split -\n",
        "\n",
        "Information Gain is the primary criterion that guides the construction of an Entropy-based Decision Tree (a greedy algorithm).Its importance for choosing the best split can be summarized in three points:\n",
        "\n",
        "Maximizing Purity: The core objective of building a classification tree is to create pure leaf nodes (where all samples belong to the same class). Since Entropy measures disorder (impurity), and IG measures the reduction in Entropy, maximizing Information Gain directly translates to finding the split that creates the most homogeneous (purest) subsets.\n",
        "\n",
        "Feature Selection: At every internal node, the Decision Tree algorithm calculates the Information Gain for every available feature. It then selects the feature with the highest Information Gain to be the splitting attribute for that node. This ensures the algorithm selects the most informative and predictive feature at each step.\n",
        "\n",
        "Efficiency of Classification: By prioritizing splits that yield the largest reduction in uncertainty, the tree structure grows in the most efficient way possible, often leading to a smaller, simpler tree that can classify new data points with fewer decisions.In short, Information Gain ensures that at every decision point, the algorithm chooses the question (the feature) that provides the most clarification about the final class label.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans - 1. Real-World Applications\n",
        "\n",
        "Decision Trees are widely used in various fields due to their simplicity and interpretability. Common applications include:\n",
        "\n",
        "a. Finance:\n",
        "\n",
        "Used for credit scoring, loan approval, and fraud detection by classifying customers based on risk profiles.\n",
        "\n",
        "b. Healthcare:\n",
        "\n",
        "Employed for disease diagnosis and treatment recommendation, using patient data (symptoms, test results, history).\n",
        "\n",
        "c. Marketing & Sales:\n",
        "\n",
        "Helps in customer segmentation, churn prediction, and targeted marketing campaigns.\n",
        "\n",
        "d. Manufacturing & Operations:\n",
        "\n",
        "Used for quality control, fault diagnosis, and predictive maintenance.\n",
        "\n",
        "e. Education:\n",
        "\n",
        "Applied to predict student performance and identify factors influencing success or dropout rates.\n",
        "\n",
        "2. Advantages -\n",
        "\n",
        "Easy to interpret and visualize: The tree structure is intuitive, making results understandable even for non-experts.\n",
        "\n",
        "Handles both numerical and categorical data: Flexible across various data types.\n",
        "\n",
        "Requires little data preprocessing: No need for normalization or scaling.\n",
        "\n",
        "Works well for small to medium datasets.\n",
        "\n",
        "3. Limitations -\n",
        "\n",
        "Overfitting: Trees can become too complex, capturing noise instead of patterns.\n",
        "\n",
        "Instability: Small changes in data can result in a completely different tree.\n",
        "\n",
        "Bias toward dominant classes: Especially if the dataset is imbalanced.\n",
        "\n",
        "Less effective with continuous variables: Compared to some other models like linear regression.\n",
        "\n",
        "Dataset Info:\n",
        "\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "Ans - Python Program: Decision Tree on Iris Dataset (Using Gini Criterion)\n",
        "\n"
      ],
      "metadata": {
        "id": "PjNYdvoSiqjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data              # Features\n",
        "y = iris.target            # Labels\n",
        "\n",
        "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print the model’s accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy: {:.2f}%\".format(accuracy * 100))\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUBLIqpCrTe0",
        "outputId": "6f265872-cba2-4f7e-f79a-0b36139bd9e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "\n",
        "Ans - Python Program: Compare Decision Trees (max_depth=3 vs Fully Grown)\n"
      ],
      "metadata": {
        "id": "GlCl7cLqrgwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree with max_depth=3\n",
        "limited_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train a fully-grown Decision Tree (no max_depth limit)\n",
        "full_tree = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions and calculate accuracy for both models\n",
        "y_pred_limited = limited_tree.predict(X_test)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 6. Print accuracy results\n",
        "print(\"Accuracy (Decision Tree with max_depth=3): {:.2f}%\".format(acc_limited * 100))\n",
        "print(\"Accuracy (Fully-grown Decision Tree): {:.2f}%\".format(acc_full * 100))\n",
        "\n",
        "# 7. (Optional) Compare model complexity\n",
        "print(\"\\nFeature Importances (max_depth=3):\", limited_tree.feature_importances_)\n",
        "print(\"Feature Importances (Fully-grown):\", full_tree.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ-MLCmProBg",
        "outputId": "aafb587d-8c18-41d0-db6d-e32bb162846e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (Decision Tree with max_depth=3): 100.00%\n",
            "Accuracy (Fully-grown Decision Tree): 100.00%\n",
            "\n",
            "Feature Importances (max_depth=3): [0.         0.         0.93462632 0.06537368]\n",
            "Feature Importances (Fully-grown): [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "\n",
        "● Load the Boston Housing Dataset\n",
        "\n",
        "● Train a Decision Tree Regressor\n",
        "\n",
        "● Print the Mean Squared Error (MSE) and feature importances.\n",
        "\n",
        "Ans - Python Program: Decision Tree Regressor on Boston Housing Dataset"
      ],
      "metadata": {
        "id": "xE-fVYdsrrQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the Boston Housing dataset (compatible with sklearn >= 1.2)\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion='squared_error', random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE): {:.2f}\".format(mse))\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWtaSVqrsTiP",
        "outputId": "3ac1b329-bb21-4c55-e2f8-0301f2d52fee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.42\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "Ans - Python Program: Hyperparameter Tuning of Decision Tree (Iris Dataset)"
      ],
      "metadata": {
        "id": "3CMOzWWesZyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. Use GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,             # 5-fold cross-validation\n",
        "    n_jobs=-1         # Use all CPU cores for faster computation\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print the best parameters\n",
        "print(\"Best Parameters Found:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# 7. Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nTest Set Accuracy: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w40_wqd4skSB",
        "outputId": "eabd267d-b1cc-42fd-8a33-c9f2dbf8f954"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found:\n",
            "{'max_depth': 4, 'min_samples_split': 2}\n",
            "\n",
            "Test Set Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Ans - 1) Handle missing values\n",
        "\n",
        "Understand the missingness\n",
        "\n",
        "Check patterns (missing completely at random / at random / not at random).\n",
        "\n",
        "Compute % missing per column and visualize (heatmap, bar chart).\n",
        "\n",
        "Decide strategy per feature type / meaning\n",
        "\n",
        "Numerical: impute with median (robust) or use model-based imputation (KNN/IterativeImputer) if relationships exist. Create a binary “was_missing” indicator if missingness might be predictive.\n",
        "\n",
        "Categorical: impute with a new category \"Missing\" or the most frequent category. For ordinal categories, consider a sensible ordered fill.\n",
        "\n",
        "If missingness is informative: keep an indicator column.\n",
        "\n",
        "Use pipelined imputers so preprocessing is reproducible and used identically at train/test/deploy time.\n",
        "\n",
        "2) Encode categorical features\n",
        "\n",
        "Low-cardinality nominal (e.g., sex, smoking_status): use OneHotEncoder (with handle_unknown='ignore').\n",
        "\n",
        "High-cardinality nominal: consider Target or LeaveOneOut encoding, or frequency encoding, or embedding approaches—careful to avoid leakage (use CV for target encoding).\n",
        "\n",
        "Ordinal (e.g., stage: low/medium/high): use OrdinalEncoder with the defined order.\n",
        "\n",
        "Pipeline + ColumnTransformer: encode different columns in one pipeline so transforms are applied consistently.\n",
        "\n",
        "3) Train a Decision Tree model\n",
        "\n",
        "Key considerations:\n",
        "\n",
        "Trees handle mixed features natively but still require non-missing values; encoding + imputation is required.\n",
        "\n",
        "For healthcare problems with class imbalance, set class_weight='balanced' or use sample weights.\n",
        "\n",
        "Minimal pipeline example (scikit-learn):"
      ],
      "metadata": {
        "id": "SsoCGmSmsq-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load dataset (simulating healthcare data using Iris)\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# --- Simulate some missing values to show handling ---\n",
        "rng = np.random.default_rng(42)\n",
        "missing_mask = rng.choice([True, False], size=X.shape, p=[0.05, 0.95])\n",
        "X = X.mask(missing_mask)\n",
        "\n",
        "# 2. Identify numerical and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_features = []  # Iris has no categorical features, but you could add some\n",
        "\n",
        "# 3. Define preprocessing for numeric and categorical columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median'))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# 4. Define Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 5. Create full pipeline\n",
        "clf = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', dt)\n",
        "])\n",
        "\n",
        "# 6. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 7. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [2, 3, 4, 5, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# 8. Run GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 9. Evaluate model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy: {:.2f}%\".format(acc * 100))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CusvHAZxtU6j",
        "outputId": "28c15f9a-dc5f-4b5d-e6f2-6c0525f8061e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__max_depth': 2, 'classifier__min_samples_split': 2}\n",
            "Test Accuracy: 93.33%\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       0.90      0.90      0.90        10\n",
            "           2       0.90      0.90      0.90        10\n",
            "\n",
            "    accuracy                           0.93        30\n",
            "   macro avg       0.93      0.93      0.93        30\n",
            "weighted avg       0.93      0.93      0.93        30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Tune hyperparameters\n",
        "\n",
        "Which hyperparameters matter most for trees: max_depth, min_samples_split, min_samples_leaf, max_features, criterion (gini/entropy), class_weight.\n",
        "\n",
        "Use cross-validation with a pipeline inside GridSearchCV or RandomizedSearchCV. Prefer RandomizedSearchCV if grid is large.\n",
        "\n",
        "Scoring: for imbalanced healthcare tasks, use roc_auc, average_precision (PR AUC), and also consider recall (sensitivity) if missing disease is very costly."
      ],
      "metadata": {
        "id": "q69xzfVvtlDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Evaluate model performance\n",
        "\n",
        "Hold-out test set (never touched during training/tuning). If possible, keep a final external validation dataset (different hospital or time period).\n",
        "\n",
        "Metrics to report (healthcare focus):\n",
        "\n",
        "ROC AUC and PR AUC (PR AUC is more informative with class imbalance).\n",
        "\n",
        "Sensitivity (Recall) for disease positive class — how many sick patients you detect.\n",
        "\n",
        "Specificity, Precision, F1.\n",
        "\n",
        "Confusion matrix and decision thresholds (don’t just use the default 0.5).\n",
        "\n",
        "Calibration (calibration plot, Brier score) — how well predicted probabilities match observed risk.\n",
        "\n",
        "Explainability & fairness:\n",
        "\n",
        "Feature importance (feature_importances_) and tree visualization (plot_tree) for interpretability.\n",
        "\n",
        "Local explanations (SHAP or LIME) for individual predictions — critical in medicine.\n",
        "\n",
        "Check performance across subgroups (age, sex, ethnicity) to detect bias.\n",
        "\n",
        "Uncertainty & robustness:\n",
        "\n",
        "Confidence intervals via bootstrap.\n",
        "\n",
        "Sensitivity analyses (e.g., varying imputation method, sample weighting).\n",
        "\n",
        "Clinical utility:\n",
        "\n",
        "Decision curve analysis and net benefit if the model will guide actions.\n",
        "\n",
        "6) Deployment & monitoring (brief)\n",
        "\n",
        "Lock preprocessing (imputers/encoders) into the model artifact (pipeline).\n",
        "\n",
        "Data validation at ingest (schema checks, missingness alerts).\n",
        "\n",
        "Monitor data drift and performance drift; set retraining triggers.\n",
        "\n",
        "Logging for predictions and human overrides.\n",
        "\n",
        "Clinical validation and pilot study before full deployment.\n",
        "\n",
        "Privacy & security: HIPAA/GDPR compliance, encryption, access controls.\n",
        "\n",
        "7) Ethical, legal & practical safeguards\n",
        "\n",
        "Human-in-the-loop: clinicians should review model outputs; model shouldn't be the sole decision-maker.\n",
        "\n",
        "Informed consent, data provenance, and explainability for decisions affecting patients.\n",
        "\n",
        "Regulatory checks: if used for diagnosis, may require medical device approval (varies by jurisdiction).\n",
        "\n",
        "8) Business value (real-world)\n",
        "\n",
        "Early detection / triage: prioritize patients who need immediate attention — improves outcomes and reduces adverse events.\n",
        "\n",
        "Resource allocation: optimize lab tests, imaging, and specialist referrals; reduce unnecessary procedures.\n",
        "\n",
        "Operational efficiency: reduce clinician time for low-risk cases, allowing focus on complex patients.\n",
        "\n",
        "Cost savings: early treatment can reduce downstream heavy costs from advanced disease.\n",
        "\n",
        "Personalized care: enable risk-stratified pathways (e.g., more frequent monitoring for high-risk groups).\n",
        "\n",
        "Monitoring & public health: aggregate predictions across population to spot outbreaks or demographic trends."
      ],
      "metadata": {
        "id": "yVJPg3JBtsOd"
      }
    }
  ]
}